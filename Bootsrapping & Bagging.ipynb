{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dba523e",
   "metadata": {},
   "source": [
    "### Ensemble learning\n",
    "\n",
    "multiple models (often of the same type) are trained to solve the same problem and combined to get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38506c59",
   "metadata": {},
   "source": [
    "**Bootstrapping:** This is a resampling technique used to estimate statistics on a dataset by sampling with replacement. It involves randomly selecting a subset of data for a model and repeating this process a number of times. \n",
    "\n",
    "Each subset can be used to train a separate model, and the results can be averaged (for regression) or voted upon (for classification) to produce a final model. \n",
    "\n",
    "Application:\n",
    "- dataset imbalance: generate new samples for classes are rare\n",
    "- ensemble learning: bagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f447c",
   "metadata": {},
   "source": [
    "**Bagging:** Short for Bootstrap Aggregating, bagging is an extension of the bootstrapping technique and is used to improve the stability and accuracy of machine learning algorithms. \n",
    "\n",
    "It involves creating multiple versions of a predictor and using these to get an aggregated predictor. Each model in a bagging ensemble is trained on a randomly drawn subset of the training dataset with replacement (i.e., bootstrapped sample). \n",
    "\n",
    "After training, predictions from each model are combined (typically by averaging or majority vote) to form a final prediction. \n",
    "\n",
    "Bagging is effective because it both reduces the variance of the prediction (by averaging) and also reduces the likelihood of overfitting.\n",
    "\n",
    "Application:\n",
    "- random forest\n",
    "- AdaBoost\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2bdbf",
   "metadata": {},
   "source": [
    "**In essence, bootstrapping is random sampling with replacement from the available training data. Bagging (= bootstrap aggregation) is performing it many times and training an estimator for each bootstrapped dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7196df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval for the mean: 4.80 to 8.40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([4, 7, 2, 9, 5, 10, 3, 6, 8, 11])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# Function to draw bootstrap samples and compute their mean\n",
    "def bootstrap(data, n_bootstrap_samples):\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    sample_means = []\n",
    "    for _ in range(n_bootstrap_samples):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        sample_means.append(np.mean(sample))\n",
    "    return np.array(sample_means)\n",
    "\n",
    "# Compute bootstrap samples\n",
    "bootstrap_means = bootstrap(data, n_bootstrap_samples)\n",
    "\n",
    "# Compute 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% confidence interval for the mean: {lower_bound:.2f} to {upper_bound:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb83fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Create a Bagging Classifier with Decision Trees\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da919a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
